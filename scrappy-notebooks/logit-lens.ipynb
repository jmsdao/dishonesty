{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handy snippet to get repo root from anywhere in the repo\n",
    "import sys\n",
    "from subprocess import check_output\n",
    "ROOT = check_output('git rev-parse --show-toplevel', shell=True).decode(\"utf-8\").strip()\n",
    "if ROOT not in sys.path: sys.path.append(ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import einops\n",
    "\n",
    "from functools import partial\n",
    "from dishonesty.mistral_lens import load_model\n",
    "from dishonesty.prompts import PROMPTS\n",
    "from dishonesty.utils import ntensor_to_long\n",
    "\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "device = t.device('cuda:0' if t.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c733e7d5a0234b63a3b879d6882e5ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4096])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "directions = t.load(f\"{ROOT}/directions/honesty_mistral-instruct-v0.1.pt\").to(device)\n",
    "directions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define injection hook\n",
    "def inject(module, input, output, alpha=-8.25):\n",
    "    new_output = [o for o in output]\n",
    "    new_output[0] += alpha * directions[15]\n",
    "    return tuple(new_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activations to cache\n",
    "cache_names = [\"embed_tokens\", \"final_rscale\"]\n",
    "for i in range(32):\n",
    "    cache_names.append(f\"attn_out_{i}\")\n",
    "    cache_names.append(f\"mlp_out_{i}\")\n",
    "\n",
    "# Run with cache for all alphas\n",
    "caches = []\n",
    "alphas = np.linspace(-8.25, 0, 12)\n",
    "for alpha in alphas:\n",
    "    model.reset_hooks()\n",
    "    model.add_hook(f\"resid_post_15\", partial(inject, alpha=alpha))\n",
    "    _, cache = model.run_with_cache(PROMPTS, cache_names)\n",
    "    caches.append(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect activations needed for logit lens\n",
    "decomp_resids = []\n",
    "final_rscales = []\n",
    "\n",
    "for cache in caches:\n",
    "    # Collect decomposed resids\n",
    "    resid, labels = cache.get_decomposed_resid_stack(return_labels=True)  # [comp, batch, pos, d_model]\n",
    "    resid = resid[:, :, -1, :]  # [comp, batch, d_model], keep only final pos\n",
    "    decomp_resids.append(resid)\n",
    "    # Collect final rscales\n",
    "    final_rscale = cache[\"final_rscale\"][None, :, -1, :]  # [1, batch, 1], keep only final pos\n",
    "    final_rscales.append(final_rscale)\n",
    "\n",
    "decomp_resids = t.stack(decomp_resids, dim=0)  # [alpha, comp, batch, d_model]\n",
    "final_rscales = t.stack(final_rscales, dim=0)  # [alpha, 1, batch, 1]\n",
    "\n",
    "assert decomp_resids.dim() == 4\n",
    "assert final_rscales.dim() == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rms_norm(resid, rscale, weight):\n",
    "    input_dtype = resid.dtype\n",
    "    scaled_resid = rscale * resid.to(rscale.dtype) * weight.to(rscale.dtype)\n",
    "    return scaled_resid.to(input_dtype)\n",
    "\n",
    "# Apply final rms norm to decompsed resids\n",
    "scaled_decomp_resids = apply_rms_norm(\n",
    "    decomp_resids,\n",
    "    final_rscales,\n",
    "    model.hf_model.model.norm.weight,\n",
    ")  # [alpha, comp, batch, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dishonest tokens\n",
    "model.reset_hooks()\n",
    "model.add_hook(f\"resid_post_15\", partial(inject, alpha=-8.25))\n",
    "dishonest_logits = model(PROMPTS)[:, -1]  # [batch, d_vocab]\n",
    "dishonest_tokens = dishonest_logits.argmax(dim=-1)  # [batch]\n",
    "\n",
    "# Get honest tokens\n",
    "model.reset_hooks()\n",
    "honest_logits = model(PROMPTS)[:, -1]  # [batch, d_vocab]\n",
    "honest_tokens = honest_logits.argmax(dim=-1)  # [batch]\n",
    "\n",
    "# Get logit diff directions\n",
    "W_U = model.hf_model.model.lm_head.weight  # [d_vocab, d_model]\n",
    "logit_diff_directions = W_U[dishonest_tokens] - W_U[honest_tokens]  # [batch, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate logit diffs\n",
    "logit_diffs_decomp = einops.einsum(\n",
    "    scaled_decomp_resids,\n",
    "    logit_diff_directions,\n",
    "    \"alpha comp batch d_model, batch d_model -> alpha comp batch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to long format\n",
    "df = ntensor_to_long(\n",
    "    logit_diffs_decomp,\n",
    "    value_name=\"logit_diff\",\n",
    "    dim_names=[\"alpha\", \"comp\", \"batch\"],\n",
    ")\n",
    "\n",
    "# Map integer index to the alpha value\n",
    "alpha_map = {i: alpha for i, alpha in enumerate(alphas)}\n",
    "df[\"alpha\"] = df[\"alpha\"].map(alpha_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    df,\n",
    "    x=\"comp\",\n",
    "    y=\"logit_diff\",\n",
    "    color=\"alpha\",\n",
    "    animation_frame=\"batch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
