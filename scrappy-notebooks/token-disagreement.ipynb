{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handy snippet to get repo root from anywhere in the repo\n",
    "import sys\n",
    "from subprocess import check_output\n",
    "ROOT = check_output('git rev-parse --show-toplevel', shell=True).decode(\"utf-8\").strip()\n",
    "if ROOT not in sys.path: sys.path.append(ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "from functools import partial\n",
    "from dishonesty.mistral_lens import load_model\n",
    "from dishonesty.utils import calc_soft_kl_div\n",
    "from dishonesty.prompts import PROMPTS\n",
    "\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "device = t.device('cuda:0' if t.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c733e7d5a0234b63a3b879d6882e5ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4096])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directions = t.load(f\"{ROOT}/directions/honesty_mistral-instruct-v0.1.pt\").to(device)\n",
    "directions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dishonesty_hooks(model):\n",
    "    # Setup dishonesty injection parameters\n",
    "    alpha_layer_pairs = [\n",
    "        (-8.25, 15),\n",
    "    ]\n",
    "\n",
    "    # Define injection hook\n",
    "    def inject(module, input, output, alpha_layer_pair=(-8.25, 15)):\n",
    "        alpha, layer = alpha_layer_pair\n",
    "        new_output = [o for o in output]\n",
    "        new_output[0] += alpha * directions[layer]\n",
    "        return tuple(new_output)\n",
    "\n",
    "    # Add the hooks\n",
    "    for alpha, layer in alpha_layer_pairs:\n",
    "        partialed_hook = partial(inject, alpha_layer_pair=(alpha, layer))\n",
    "        model.add_hook(f\"resid_post_{layer}\", partialed_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generation parameters\n",
    "max_new_tokens = 48\n",
    "input_tokens, mask = model.to_tokens(PROMPTS, return_mask=True)\n",
    "\n",
    "# Generate tokens without dishonesty injection\n",
    "model.reset_hooks()\n",
    "output_tokens_honest = model.hf_model.generate(\n",
    "    input_ids=input_tokens,\n",
    "    attention_mask=mask,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "# Generate tokens WITH dishonesty injection\n",
    "model.reset_hooks()\n",
    "add_dishonesty_hooks(model)\n",
    "output_tokens_dishonest = model.hf_model.generate(\n",
    "    input_ids=input_tokens,\n",
    "    attention_mask=mask,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "# Get tokenized strings\n",
    "dishonest_tokenized_strings = np.array(model.to_string_tokenized(output_tokens_dishonest))\n",
    "honest_tokenized_strings = np.array(model.to_string_tokenized(output_tokens_honest))\n",
    "\n",
    "# Get honest logits\n",
    "model.reset_hooks()\n",
    "honest_logits_on_dishonest_tokens = model(output_tokens_dishonest)\n",
    "honest_logits_on_honest_tokens = model(output_tokens_honest)\n",
    "\n",
    "# Get dishonest logits\n",
    "model.reset_hooks()\n",
    "add_dishonesty_hooks(model)\n",
    "dishonest_logits_on_dishonest_tokens = model(output_tokens_dishonest)\n",
    "dishonest_logits_on_honest_tokens = model(output_tokens_honest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_disagreement_heatmap(\n",
    "    model,\n",
    "    main_logits,\n",
    "    alt_logits,\n",
    "    main_text,\n",
    "    start_pos,\n",
    "    end_pos,\n",
    "    temperature=0.0,\n",
    "):\n",
    "    # Compute metrics\n",
    "    metrics = calc_soft_kl_div(main_logits, alt_logits, temperature=temperature)\n",
    "    metrics = metrics.roll(1, dims=-1)\n",
    "\n",
    "    # Get alt text for hover\n",
    "    alt_text = main_text.copy()\n",
    "    argmaxes = alt_logits.argmax(dim=-1).clone()\n",
    "    argmaxes = argmaxes.roll(1, dims=-1)\n",
    "    argmaxes[:, 0] = 0\n",
    "    batch_size, seq_len = main_text.shape\n",
    "    for b in range(batch_size):\n",
    "        for p in range(seq_len):\n",
    "            alt_text[b, p] = model.tokenizer.decode(argmaxes[b, p].item())\n",
    "\n",
    "    # Create the heatmap\n",
    "    fig = px.imshow(\n",
    "        metrics.cpu().numpy()[:, start_pos:end_pos],\n",
    "        color_continuous_scale=\"RdBu_r\",\n",
    "        color_continuous_midpoint=0,\n",
    "        width=1700,\n",
    "        height=600,\n",
    "        aspect=\"auto\",\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        text=main_text[:, start_pos:end_pos],\n",
    "        texttemplate=\"%{text}\",\n",
    "        hovertext=alt_text[:, start_pos:end_pos],\n",
    "        textfont_size=13,\n",
    "        hovertemplate=(\n",
    "            \"Alt Token: |%{hovertext}|<br>\"\n",
    "            \"Metric: %{z}<br>\"\n",
    "            \"Batch: %{y}<br>\"\n",
    "            \"Pos: %{x}<br>\"\n",
    "        ),\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(\n",
    "            tickmode='array',\n",
    "            tickvals=list(range(end_pos - start_pos)),\n",
    "            ticktext=[str(i) for i in range(start_pos, end_pos)],\n",
    "        ),\n",
    "        xaxis_title=\"Token Position\",\n",
    "        yaxis_title=\"Batch\",\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_disagreement_heatmap(\n",
    "    model,\n",
    "    dishonest_logits_on_dishonest_tokens,\n",
    "    honest_logits_on_dishonest_tokens,\n",
    "    dishonest_tokenized_strings,\n",
    "    35,\n",
    "    64,\n",
    "    temperature=0.0,\n",
    ")\n",
    "fig.layout.coloraxis.colorbar.title = \"Soft KL Div<br>(T=0)\"\n",
    "fig.layout.title = \"Main: dishonest_logprobs_on_dishonest_tokens<br>Alt: honest_logprobs_on_dishonest_tokens\"\n",
    "fig.write_html(f\"{ROOT}/figs/disagreement-heatmap1.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
